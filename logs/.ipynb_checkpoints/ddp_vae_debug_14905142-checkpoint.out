[2025-07-14 03:02:18] Starting SLURM job 14905142 on clara10.sc.uni-leipzig.de
[2025-07-14 03:02:18] Job parameters: 4 tasks, 4 GPUs, 1 nodes
[2025-07-14 03:02:18] Loading required modules...
[2025-07-14 03:02:20] Activating conda environment...
[2025-07-14 03:02:21] Verifying environment...
Python 3.10.18
PyTorch version: 2.7.1+cu126
CUDA available: True
CUDA devices: 4
[2025-07-14 03:02:29] Checking training script...
[2025-07-14 03:02:29] Training script found: -rw-r--r-- 1 ci72buri sc_users 17203 Jul 14 02:58 train_vae.py
[2025-07-14 03:02:29] Testing training script import...
Training script import: SUCCESS
[2025-07-14 03:02:36] Checking required modules...
torch: OK
torch.distributed: OK
numpy: OK
wandb: OK
All required modules available
[2025-07-14 03:02:40] Configuring NCCL settings...
[2025-07-14 03:02:40] Setting up distributed training configuration...
[2025-07-14 03:02:40] Distributed training configuration:
[2025-07-14 03:02:40]   MASTER_ADDR: 172.26.10.10
[2025-07-14 03:02:40]   MASTER_PORT: 58952
[2025-07-14 03:02:40]   WORLD_SIZE: 4
[2025-07-14 03:02:40]   NODE_RANK: 0
[2025-07-14 03:02:40]   SLURM_GPUS_ON_NODE: 4
[2025-07-14 03:02:40]   SLURM_JOB_NUM_NODES: 1
[2025-07-14 03:02:40] Testing network connectivity...
PING 172.26.10.10 (172.26.10.10) 56(84) bytes of data.
64 bytes from 172.26.10.10: icmp_seq=1 ttl=64 time=0.060 ms

--- 172.26.10.10 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.060/0.060/0.060/0.000 ms
[2025-07-14 03:02:40] Testing GPU availability...
Mon Jul 14 03:02:40 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:05:00.0 Off |                  N/A |
|  0%   28C    P8              21W / 250W |      1MiB / 11264MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:06:00.0 Off |                  N/A |
|  0%   27C    P8               1W / 250W |      1MiB / 11264MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:28:00.0 Off |                  N/A |
|  0%   26C    P8              20W / 250W |      1MiB / 11264MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:29:00.0 Off |                  N/A |
|  0%   26C    P8              20W / 250W |      1MiB / 11264MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
[2025-07-14 03:02:40] Checking torchrun availability...
/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/bin/torchrun
usage: torchrun [-h] [--nnodes NNODES] [--nproc-per-node NPROC_PER_NODE] [--rdzv-backend RDZV_BACKEND] [--rdzv-endpoint RDZV_ENDPOINT] [--rdzv-id RDZV_ID] [--rdzv-conf RDZV_CONF] [--standalone]
                [--max-restarts MAX_RESTARTS] [--monitor-interval MONITOR_INTERVAL] [--start-method {spawn,fork,forkserver}] [--role ROLE] [-m] [--no-python] [--run-path] [--log-dir LOG_DIR] [-r REDIRECTS] [-t TEE]
                [--local-ranks-filter LOCAL_RANKS_FILTER] [--node-rank NODE_RANK] [--master-addr MASTER_ADDR] [--master-port MASTER_PORT] [--local-addr LOCAL_ADDR] [--logs-specs LOGS_SPECS]
                training_script ...

[2025-07-14 03:02:43] Testing basic torchrun functionality...
Basic torchrun test passed
[2025-07-14 03:02:48] Creating distributed test script...
[2025-07-14 03:02:48] Testing distributed functionality...
Rank 0/2: Distributed initialization successfulRank 1/2: Distributed initialization successful

Rank 0: Using device cuda:0
Rank 1: Using device cuda:1
Rank 1: Basic tensor operations successful
Rank 0: Basic tensor operations successful
Rank 1: Test completed successfully
Rank 0: Test completed successfully
[2025-07-14 03:02:55] Starting distributed training with verbose output...
[2025-07-14 03:02:55] Command: torchrun --nproc_per_node=4 --nnodes=1 --node_rank=0 --rdzv_backend=c10d --rdzv_endpoint=172.26.10.10:58952 train_vae.py
[2025-07-14 03:02:55] Testing with 2 processes first...
[W714 03:02:57.466168666 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [clara10.sc.uni-leipzig.de]:58952 (errno: 97 - Address family not supported by protocol).
[W714 03:02:57.586498028 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [clara10.sc.uni-leipzig.de]:42709 (errno: 97 - Address family not supported by protocol).
[W714 03:03:03.118326993 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [clara10.sc.uni-leipzig.de]:42709 (errno: 97 - Address family not supported by protocol).
[W714 03:03:03.171860072 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [clara10.sc.uni-leipzig.de]:42709 (errno: 97 - Address family not supported by protocol).
2025-07-14 03:03:03,438 - rank_0 - INFO - Rank 0/2 initialized on device cuda:0
wandb: Currently logged in as: coyfelix7 (coyfelix7-universit-t-leipzig) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Using VAE-optimized preprocessing...
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/sc.uni-leipzig.de/ci72buri/projects/vae/Leipzig_big_data_praktikum_gene_expression_autoencoder/wandb/run-20250714_030303-xjwezsmm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-durian-168
wandb: ‚≠êÔ∏è View project at https://wandb.ai/coyfelix7-universit-t-leipzig/big_data_vae
wandb: üöÄ View run at https://wandb.ai/coyfelix7-universit-t-leipzig/big_data_vae/runs/xjwezsmm
Using VAE-optimized preprocessing...
Dataset loaded: 20000 cells √ó 100 genes
/home/sc.uni-leipzig.de/ci72buri/projects/vae/Leipzig_big_data_praktikum_gene_expression_autoencoder/autoCell/data_loader.py:152: ImplicitModificationWarning: Modifying `X` on a view results in data being overridden
  self.adata.X = X
Dataset loaded: 20000 cells √ó 100 genes
/home/sc.uni-leipzig.de/ci72buri/projects/vae/Leipzig_big_data_praktikum_gene_expression_autoencoder/autoCell/data_loader.py:152: ImplicitModificationWarning: Modifying `X` on a view results in data being overridden
  self.adata.X = X
