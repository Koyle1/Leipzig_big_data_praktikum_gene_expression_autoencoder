[2025-07-14 12:29:57] Starting SLURM job 14912024 on clara07.sc.uni-leipzig.de
[2025-07-14 12:29:57] Job parameters: 4 tasks, 4 GPUs, 1 nodes
[2025-07-14 12:29:57] Loading required modules...
[2025-07-14 12:29:59] Activating conda environment...
[2025-07-14 12:29:59] Verifying environment...
Python 3.10.18
PyTorch version: 2.7.1+cu126
CUDA available: True
CUDA devices: 4
[2025-07-14 12:30:08] Checking training script...
[2025-07-14 12:30:08] Training script found: -rw-r--r-- 1 ci72buri sc_users 20831 Jul 14 12:29 train_vae.py
[2025-07-14 12:30:08] Testing training script import...
Training script import: FAILED - No module named 'psutil'
[2025-07-14 12:30:13] Checking required modules...
torch: OK
torch.distributed: OK
numpy: OK
wandb: OK
All required modules available
[2025-07-14 12:30:17] Configuring NCCL settings...
[2025-07-14 12:30:17] Setting up distributed training configuration...
[2025-07-14 12:30:17] Distributed training configuration:
[2025-07-14 12:30:17]   MASTER_ADDR: 172.26.10.7
[2025-07-14 12:30:17]   MASTER_PORT: 34029
[2025-07-14 12:30:17]   WORLD_SIZE: 4
[2025-07-14 12:30:17]   NODE_RANK: 0
[2025-07-14 12:30:17]   SLURM_GPUS_ON_NODE: 4
[2025-07-14 12:30:17]   SLURM_JOB_NUM_NODES: 1
[2025-07-14 12:30:17] Testing network connectivity...
PING 172.26.10.7 (172.26.10.7) 56(84) bytes of data.
64 bytes from 172.26.10.7: icmp_seq=1 ttl=64 time=0.047 ms

--- 172.26.10.7 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.047/0.047/0.047/0.000 ms
[2025-07-14 12:30:17] Testing GPU availability...
Mon Jul 14 12:30:17 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla V100-PCIE-32GB           On  | 00000000:05:00.0 Off |                    0 |
| N/A   30C    P0              26W / 250W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           On  | 00000000:29:00.0 Off |                    0 |
| N/A   25C    P0              25W / 250W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           On  | 00000000:43:00.0 Off |                    0 |
| N/A   27C    P0              25W / 250W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           On  | 00000000:64:00.0 Off |                    0 |
| N/A   30C    P0              26W / 250W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
[2025-07-14 12:30:17] Checking torchrun availability...
/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/bin/torchrun
usage: torchrun [-h] [--nnodes NNODES] [--nproc-per-node NPROC_PER_NODE] [--rdzv-backend RDZV_BACKEND] [--rdzv-endpoint RDZV_ENDPOINT] [--rdzv-id RDZV_ID] [--rdzv-conf RDZV_CONF] [--standalone]
                [--max-restarts MAX_RESTARTS] [--monitor-interval MONITOR_INTERVAL] [--start-method {spawn,fork,forkserver}] [--role ROLE] [-m] [--no-python] [--run-path] [--log-dir LOG_DIR] [-r REDIRECTS] [-t TEE]
                [--local-ranks-filter LOCAL_RANKS_FILTER] [--node-rank NODE_RANK] [--master-addr MASTER_ADDR] [--master-port MASTER_PORT] [--local-addr LOCAL_ADDR] [--logs-specs LOGS_SPECS]
                training_script ...

[2025-07-14 12:30:20] Testing basic torchrun functionality...
Basic torchrun test passed
[2025-07-14 12:30:25] Creating distributed test script...
[2025-07-14 12:30:25] Testing distributed functionality...
Rank 0/2: Distributed initialization successful
Rank 1/2: Distributed initialization successful
Rank 0: Using device cuda:0
Rank 1: Using device cuda:1
Rank 0: Basic tensor operations successful
Rank 1: Basic tensor operations successful
Rank 1: Test completed successfully
Rank 0: Test completed successfully
[2025-07-14 12:30:32] Starting distributed training with verbose output...
[2025-07-14 12:30:32] Command: torchrun --nproc_per_node=4 --nnodes=1 --node_rank=0 --rdzv_backend=c10d --rdzv_endpoint=172.26.10.7:34029 train_vae.py
[2025-07-14 12:30:32] Testing with 2 processes first...
[W714 12:30:34.425200589 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [clara07.sc.uni-leipzig.de]:34029 (errno: 97 - Address family not supported by protocol).
[W714 12:30:35.723342131 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [clara07.sc.uni-leipzig.de]:46049 (errno: 97 - Address family not supported by protocol).
Traceback (most recent call last):
  File "/home/sc.uni-leipzig.de/ci72buri/projects/vae/Leipzig_big_data_praktikum_gene_expression_autoencoder/train_vae.py", line 21, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'
Traceback (most recent call last):
  File "/home/sc.uni-leipzig.de/ci72buri/projects/vae/Leipzig_big_data_praktikum_gene_expression_autoencoder/train_vae.py", line 21, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'
W0714 12:30:39.461000 3322228 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3322431 closing signal SIGTERM
E0714 12:30:39.525000 3322228 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 1 (pid: 3322432) of binary: /home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/bin/python3.10
Traceback (most recent call last):
  File "/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
  File "/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_vae.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-14_12:30:39
  host      : clara07.sc.uni-leipzig.de
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3322432)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[2025-07-14 12:30:39] Training exit code: 0
[2025-07-14 12:30:39] Training completed successfully
[2025-07-14 12:30:39] Saving system state for debugging...
[2025-07-14 12:30:40] Debug files created: debug_env_vars.txt, debug_python_libs.txt, debug_gpu_state.txt
[2025-07-14 12:30:40] Job completed with exit code 0
Cleaning up...
Cleanup completed
