MASTER_ADDR is 172.18.62.11
PING 172.18.62.11 (172.18.62.11) 56(84) bytes of data.
64 bytes from 172.18.62.11: icmp_seq=1 ttl=64 time=0.044 ms

--- 172.18.62.11 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.044/0.044/0.044/0.000 ms
W0714 23:55:05.990000 21246 site-packages/torch/distributed/run.py:766] 
W0714 23:55:05.990000 21246 site-packages/torch/distributed/run.py:766] *****************************************
W0714 23:55:05.990000 21246 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0714 23:55:05.990000 21246 site-packages/torch/distributed/run.py:766] *****************************************
[W714 23:55:05.950289778 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [paula11.sc.uni-leipzig.de]:44613 (errno: 97 - Address family not supported by protocol).
[W714 23:55:06.237503572 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [paula11.sc.uni-leipzig.de]:34311 (errno: 97 - Address family not supported by protocol).
[W714 23:55:10.273272463 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [paula11.sc.uni-leipzig.de]:34311 (errno: 97 - Address family not supported by protocol).
[W714 23:55:10.274371952 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [paula11.sc.uni-leipzig.de]:34311 (errno: 97 - Address family not supported by protocol).
[W714 23:55:10.274373435 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [paula11.sc.uni-leipzig.de]:34311 (errno: 97 - Address family not supported by protocol).
[W714 23:55:10.274551660 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [paula11.sc.uni-leipzig.de]:34311 (errno: 97 - Address family not supported by protocol).
[W714 23:55:10.274794176 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [paula11.sc.uni-leipzig.de]:34311 (errno: 97 - Address family not supported by protocol).
wandb: Currently logged in as: coyfelix7 (coyfelix7-universit-t-leipzig) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Using VAE-optimized preprocessing...
Using VAE-optimized preprocessing...
Using VAE-optimized preprocessing...
Using VAE-optimized preprocessing...
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/sc.uni-leipzig.de/ci72buri/projects/vae/Leipzig_big_data_praktikum_gene_expression_autoencoder/wandb/run-20250714_235511-ex8byn5q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-eon-192
wandb: ‚≠êÔ∏è View project at https://wandb.ai/coyfelix7-universit-t-leipzig/big_data_vae
wandb: üöÄ View run at https://wandb.ai/coyfelix7-universit-t-leipzig/big_data_vae/runs/ex8byn5q
Using VAE-optimized preprocessing...
Dataset loaded: 5000 cells √ó 2000 genes
/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 25 worker processes in total. Our suggested max number of worker in current system is 20, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset loaded: 5000 cells √ó 2000 genes
/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 25 worker processes in total. Our suggested max number of worker in current system is 20, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset loaded: 5000 cells √ó 2000 genes
/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 25 worker processes in total. Our suggested max number of worker in current system is 20, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset loaded: 5000 cells √ó 2000 genes
/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 25 worker processes in total. Our suggested max number of worker in current system is 20, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset loaded: 5000 cells √ó 2000 genes
/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 25 worker processes in total. Our suggested max number of worker in current system is 20, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
paula11:21274:21274 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond0
paula11:21274:21274 [0] NCCL INFO Bootstrap: Using bond0:172.18.62.11<0>
paula11:21274:21274 [0] NCCL INFO cudaDriverVersion 12020
paula11:21274:21274 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
paula11:21274:21274 [0] NCCL INFO Comm config Blocking set to 1
paula11:21276:21276 [2] NCCL INFO cudaDriverVersion 12020
paula11:21276:21276 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond0
paula11:21276:21276 [2] NCCL INFO Bootstrap: Using bond0:172.18.62.11<0>
paula11:21276:21276 [2] NCCL INFO NCCL version 2.26.2+cuda12.2
paula11:21276:21276 [2] NCCL INFO Comm config Blocking set to 1
paula11:21275:21275 [1] NCCL INFO cudaDriverVersion 12020
paula11:21275:21275 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond0
paula11:21275:21275 [1] NCCL INFO Bootstrap: Using bond0:172.18.62.11<0>
paula11:21275:21275 [1] NCCL INFO NCCL version 2.26.2+cuda12.2
paula11:21277:21277 [3] NCCL INFO cudaDriverVersion 12020
paula11:21277:21277 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond0
paula11:21277:21277 [3] NCCL INFO Bootstrap: Using bond0:172.18.62.11<0>
paula11:21277:21277 [3] NCCL INFO NCCL version 2.26.2+cuda12.2
paula11:21278:21278 [4] NCCL INFO cudaDriverVersion 12020
paula11:21278:21278 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond0
paula11:21275:21275 [1] NCCL INFO Comm config Blocking set to 1
paula11:21278:21278 [4] NCCL INFO Bootstrap: Using bond0:172.18.62.11<0>
paula11:21278:21278 [4] NCCL INFO NCCL version 2.26.2+cuda12.2
paula11:21277:21277 [3] NCCL INFO Comm config Blocking set to 1
paula11:21278:21278 [4] NCCL INFO Comm config Blocking set to 1
paula11:21274:21403 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
paula11:21274:21403 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
paula11:21274:21403 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond0
paula11:21274:21403 [0] NCCL INFO NET/Socket : Using [0]bond0:172.18.62.11<0>
paula11:21274:21403 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
paula11:21274:21403 [0] NCCL INFO Using network Socket
paula11:21274:21403 [0] NCCL INFO ncclCommInitRankConfig comm 0x5617e21e8ec0 rank 0 nranks 5 cudaDev 0 nvmlDev 0 busId 1000 commId 0x25c2678c4c9055a9 - Init START
paula11:21276:21404 [2] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
paula11:21276:21404 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
paula11:21276:21404 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond0
paula11:21276:21404 [2] NCCL INFO NET/Socket : Using [0]bond0:172.18.62.11<0>
paula11:21276:21404 [2] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
paula11:21276:21404 [2] NCCL INFO Using network Socket
paula11:21276:21404 [2] NCCL INFO ncclCommInitRankConfig comm 0x562e02b74b40 rank 2 nranks 5 cudaDev 2 nvmlDev 2 busId 61000 commId 0x25c2678c4c9055a9 - Init START
paula11:21275:21405 [1] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
paula11:21275:21405 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
paula11:21275:21405 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond0
paula11:21275:21405 [1] NCCL INFO NET/Socket : Using [0]bond0:172.18.62.11<0>
paula11:21278:21407 [4] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
paula11:21278:21407 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
paula11:21278:21407 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond0
paula11:21278:21407 [4] NCCL INFO NET/Socket : Using [0]bond0:172.18.62.11<0>
paula11:21277:21406 [3] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
paula11:21277:21406 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
paula11:21277:21406 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond0
paula11:21275:21405 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
paula11:21275:21405 [1] NCCL INFO Using network Socket
paula11:21277:21406 [3] NCCL INFO NET/Socket : Using [0]bond0:172.18.62.11<0>
paula11:21278:21407 [4] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
paula11:21278:21407 [4] NCCL INFO Using network Socket
paula11:21275:21405 [1] NCCL INFO ncclCommInitRankConfig comm 0x55e5a2f127b0 rank 1 nranks 5 cudaDev 1 nvmlDev 1 busId 22000 commId 0x25c2678c4c9055a9 - Init START
paula11:21277:21406 [3] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
paula11:21277:21406 [3] NCCL INFO Using network Socket
paula11:21278:21407 [4] NCCL INFO ncclCommInitRankConfig comm 0x55ccf1cbb560 rank 4 nranks 5 cudaDev 4 nvmlDev 4 busId c1000 commId 0x25c2678c4c9055a9 - Init START
paula11:21277:21406 [3] NCCL INFO ncclCommInitRankConfig comm 0x56160d12eb10 rank 3 nranks 5 cudaDev 3 nvmlDev 3 busId a1000 commId 0x25c2678c4c9055a9 - Init START
paula11:21275:21405 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
paula11:21274:21403 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
paula11:21276:21404 [2] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
paula11:21277:21406 [3] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
paula11:21278:21407 [4] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
paula11:21277:21406 [3] NCCL INFO Bootstrap timings total 0.021320 (create 0.000025, send 0.018495, recv 0.000194, ring 0.000174, delay 0.000000)
paula11:21276:21404 [2] NCCL INFO Bootstrap timings total 0.028050 (create 0.000034, send 0.012244, recv 0.013077, ring 0.000579, delay 0.000000)
paula11:21275:21405 [1] NCCL INFO Bootstrap timings total 0.024079 (create 0.000030, send 0.019249, recv 0.000214, ring 0.001980, delay 0.000000)
paula11:21278:21407 [4] NCCL INFO Bootstrap timings total 0.022471 (create 0.000025, send 0.017651, recv 0.000312, ring 0.000106, delay 0.000000)
paula11:21274:21403 [0] NCCL INFO Bootstrap timings total 0.039204 (create 0.000035, send 0.000948, recv 0.033561, ring 0.001957, delay 0.000000)
paula11:21274:21403 [0] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
paula11:21275:21405 [1] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
paula11:21274:21403 [0] NCCL INFO Setting affinity for GPU 0 to 010000,00000000
paula11:21274:21403 [0] NCCL INFO NVLS multicast support is not available on dev 0
paula11:21275:21405 [1] NCCL INFO Setting affinity for GPU 1 to 01,00000000
paula11:21275:21405 [1] NCCL INFO NVLS multicast support is not available on dev 1
paula11:21278:21407 [4] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
paula11:21278:21407 [4] NCCL INFO Setting affinity for GPU 4 to ff0000,00000000,00000000
paula11:21278:21407 [4] NCCL INFO NVLS multicast support is not available on dev 4
paula11:21276:21404 [2] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
paula11:21276:21404 [2] NCCL INFO Setting affinity for GPU 2 to 01
paula11:21276:21404 [2] NCCL INFO NVLS multicast support is not available on dev 2
paula11:21277:21406 [3] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
paula11:21277:21406 [3] NCCL INFO Setting affinity for GPU 3 to 01ff,00000000,00000000,00000000
paula11:21277:21406 [3] NCCL INFO NVLS multicast support is not available on dev 3
paula11:21278:21407 [4] NCCL INFO comm 0x55ccf1cbb560 rank 4 nRanks 5 nNodes 1 localRanks 5 localRank 4 MNNVL 0
paula11:21277:21406 [3] NCCL INFO comm 0x56160d12eb10 rank 3 nRanks 5 nNodes 1 localRanks 5 localRank 3 MNNVL 0
paula11:21275:21405 [1] NCCL INFO comm 0x55e5a2f127b0 rank 1 nRanks 5 nNodes 1 localRanks 5 localRank 1 MNNVL 0
paula11:21274:21403 [0] NCCL INFO comm 0x5617e21e8ec0 rank 0 nRanks 5 nNodes 1 localRanks 5 localRank 0 MNNVL 0
paula11:21276:21404 [2] NCCL INFO comm 0x562e02b74b40 rank 2 nRanks 5 nNodes 1 localRanks 5 localRank 2 MNNVL 0
paula11:21278:21407 [4] NCCL INFO Trees [0] -1/-1/-1->4->3 [1] -1/-1/-1->4->3
paula11:21277:21406 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
paula11:21278:21407 [4] NCCL INFO P2P Chunksize set to 131072
paula11:21277:21406 [3] NCCL INFO P2P Chunksize set to 131072
paula11:21275:21405 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
paula11:21274:21403 [0] NCCL INFO Channel 00/02 : 0 1 2 3 4
paula11:21275:21405 [1] NCCL INFO P2P Chunksize set to 131072
paula11:21276:21404 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
paula11:21274:21403 [0] NCCL INFO Channel 01/02 : 0 1 2 3 4
paula11:21276:21404 [2] NCCL INFO P2P Chunksize set to 131072
paula11:21274:21403 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
paula11:21274:21403 [0] NCCL INFO P2P Chunksize set to 131072
paula11:21274:21403 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
paula11:21278:21418 [4] NCCL INFO [Proxy Service] Device 4 CPU core 86
paula11:21278:21420 [4] NCCL INFO [Proxy Service UDS] Device 4 CPU core 87
paula11:21277:21419 [3] NCCL INFO [Proxy Service] Device 3 CPU core 104
paula11:21277:21422 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 99
paula11:21278:21407 [4] NCCL INFO threadThresholds 8/8/64 | 40/8/64 | 512 | 512
paula11:21278:21407 [4] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
paula11:21277:21406 [3] NCCL INFO threadThresholds 8/8/64 | 40/8/64 | 512 | 512
paula11:21277:21406 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
paula11:21275:21405 [1] NCCL INFO threadThresholds 8/8/64 | 40/8/64 | 512 | 512
paula11:21275:21405 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
paula11:21276:21404 [2] NCCL INFO threadThresholds 8/8/64 | 40/8/64 | 512 | 512
paula11:21276:21404 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
paula11:21274:21403 [0] NCCL INFO threadThresholds 8/8/64 | 40/8/64 | 512 | 512
paula11:21274:21403 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
paula11:21274:21427 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 48
paula11:21274:21424 [0] NCCL INFO [Proxy Service] Device 0 CPU core 48
paula11:21274:21403 [0] NCCL INFO CC Off, workFifoBytes 1048576
paula11:21277:21406 [3] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
paula11:21277:21406 [3] NCCL INFO ncclCommInitRankConfig comm 0x56160d12eb10 rank 3 nranks 5 cudaDev 3 nvmlDev 3 busId a1000 commId 0x25c2678c4c9055a9 - Init COMPLETE
paula11:21277:21406 [3] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 5 total 0.24 (kernels 0.12, alloc 0.01, bootstrap 0.02, allgathers 0.00, topo 0.08, graphs 0.00, connections 0.01, rest 0.00)
paula11:21278:21407 [4] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
paula11:21278:21407 [4] NCCL INFO ncclCommInitRankConfig comm 0x55ccf1cbb560 rank 4 nranks 5 cudaDev 4 nvmlDev 4 busId c1000 commId 0x25c2678c4c9055a9 - Init COMPLETE
paula11:21278:21407 [4] NCCL INFO Init timings - ncclCommInitRankConfig: rank 4 nranks 5 total 0.24 (kernels 0.12, alloc 0.00, bootstrap 0.02, allgathers 0.00, topo 0.08, graphs 0.00, connections 0.01, rest 0.00)
paula11:21275:21405 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
paula11:21275:21405 [1] NCCL INFO ncclCommInitRankConfig comm 0x55e5a2f127b0 rank 1 nranks 5 cudaDev 1 nvmlDev 1 busId 22000 commId 0x25c2678c4c9055a9 - Init COMPLETE
paula11:21275:21405 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 5 total 0.24 (kernels 0.12, alloc 0.00, bootstrap 0.02, allgathers 0.00, topo 0.08, graphs 0.00, connections 0.01, rest 0.00)
paula11:21276:21404 [2] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
paula11:21276:21404 [2] NCCL INFO ncclCommInitRankConfig comm 0x562e02b74b40 rank 2 nranks 5 cudaDev 2 nvmlDev 2 busId 61000 commId 0x25c2678c4c9055a9 - Init COMPLETE
paula11:21276:21404 [2] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 5 total 0.24 (kernels 0.12, alloc 0.00, bootstrap 0.03, allgathers 0.00, topo 0.08, graphs 0.00, connections 0.01, rest 0.00)
paula11:21274:21403 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
paula11:21274:21403 [0] NCCL INFO ncclCommInitRankConfig comm 0x5617e21e8ec0 rank 0 nranks 5 cudaDev 0 nvmlDev 0 busId 1000 commId 0x25c2678c4c9055a9 - Init COMPLETE
paula11:21274:21403 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 5 total 0.25 (kernels 0.12, alloc 0.00, bootstrap 0.04, allgathers 0.00, topo 0.08, graphs 0.00, connections 0.01, rest 0.00)
paula11:21275:21425 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 32
paula11:21275:21421 [1] NCCL INFO [Proxy Service] Device 1 CPU core 32
paula11:21276:21426 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 0
paula11:21276:21423 [2] NCCL INFO [Proxy Service] Device 2 CPU core 0
paula11:21278:21428 [4] NCCL INFO Channel 00 : 4[4] -> 0[0] via SHM/direct/direct
paula11:21278:21428 [4] NCCL INFO Channel 01 : 4[4] -> 0[0] via SHM/direct/direct
paula11:21277:21429 [3] NCCL INFO Channel 00 : 3[3] -> 4[4] via SHM/direct/direct
paula11:21277:21429 [3] NCCL INFO Channel 01 : 3[3] -> 4[4] via SHM/direct/direct
paula11:21275:21430 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct
paula11:21274:21432 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
paula11:21276:21431 [2] NCCL INFO Channel 00 : 2[2] -> 3[3] via SHM/direct/direct
paula11:21275:21430 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct
paula11:21274:21432 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
paula11:21276:21431 [2] NCCL INFO Channel 01 : 2[2] -> 3[3] via SHM/direct/direct
paula11:21275:21430 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
paula11:21278:21428 [4] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
paula11:21274:21432 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
paula11:21277:21429 [3] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
paula11:21276:21431 [2] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 25 worker processes in total. Our suggested max number of worker in current system is 20, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 25 worker processes in total. Our suggested max number of worker in current system is 20, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 25 worker processes in total. Our suggested max number of worker in current system is 20, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 25 worker processes in total. Our suggested max number of worker in current system is 20, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 25 worker processes in total. Our suggested max number of worker in current system is 20, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[Rank 4] Model forward completed.
[Rank 0] Model forward completed.
[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Rank 2] Model forward completed.
[Epoch 1] Time: 0.80s | Loss: 0.0108
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 0] Model forward completed.
[Rank 4] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 2] Time: 0.06s | Loss: 0.0099
[Rank 0] Model forward completed.
[Rank 2] Model forward completed.
[Rank 4] Model forward completed.
[Rank 3] Model forward completed.
[Rank 1] Model forward completed.
[Epoch 3] Time: 0.04s | Loss: 0.0019
[Rank 1] Model forward completed.
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.
[Rank 3] Model forward completed.
[Rank 0] Model forward completed.
[Epoch 4] Time: 0.05s | Loss: 0.0008
[Rank 2] Model forward completed.
[Rank 0] Model forward completed.
[Rank 1] Model forward completed.[Rank 4] Model forward completed.

[Rank 3] Model forward completed.
[Epoch 5] Time: 0.04s | Loss: 0.0008
[Rank 1] Model forward completed.
[Rank 4] Model forward completed.
[Rank 0] Model forward completed.
[Rank 2] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 6] Time: 0.04s | Loss: 0.0006
[Rank 0] Model forward completed.
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 7] Time: 0.04s | Loss: 0.0005
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.
[Rank 0] Model forward completed.
[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 8] Time: 0.04s | Loss: 0.0005
[Rank 2] Model forward completed.
[Rank 3] Model forward completed.
[Rank 0] Model forward completed.
[Rank 1] Model forward completed.
[Rank 4] Model forward completed.
[Epoch 9] Time: 0.04s | Loss: 0.0004
Epoch 10, batch size: 1000
[Rank 2] Model forward completed.
[Rank 0] Model forward completed.[Rank 4] Model forward completed.

[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 10] Time: 0.04s | Loss: 0.0004
[Rank 1] Model forward completed.
[Rank 2] Model forward completed.
[Rank 4] Model forward completed.
[Rank 0] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 11] Time: 0.04s | Loss: 0.0004
[Rank 2] Model forward completed.
[Rank 4] Model forward completed.[Rank 0] Model forward completed.

[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 12] Time: 0.04s | Loss: 0.0004
[Rank 4] Model forward completed.[Rank 0] Model forward completed.

[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 13] Time: 0.04s | Loss: 0.0003
[Rank 2] Model forward completed.
[Rank 0] Model forward completed.
[Rank 1] Model forward completed.
[Rank 4] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 14] Time: 0.04s | Loss: 0.0003
[Rank 2] Model forward completed.
[Rank 0] Model forward completed.
[Rank 4] Model forward completed.
[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 15] Time: 0.04s | Loss: 0.0003
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 0] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 16] Time: 0.04s | Loss: 0.0003
[Rank 2] Model forward completed.
[Rank 4] Model forward completed.
[Rank 3] Model forward completed.
[Rank 0] Model forward completed.
[Rank 1] Model forward completed.
[Epoch 17] Time: 0.05s | Loss: 0.0003
[Rank 2] Model forward completed.
[Rank 4] Model forward completed.
[Rank 3] Model forward completed.
[Rank 0] Model forward completed.
[Rank 1] Model forward completed.
[Epoch 18] Time: 0.05s | Loss: 0.0003
[Rank 2] Model forward completed.
[Rank 4] Model forward completed.
[Rank 0] Model forward completed.
[Rank 3] Model forward completed.
[Rank 1] Model forward completed.
[Epoch 19] Time: 0.04s | Loss: 0.0003
Epoch 20, batch size: 1000
[Rank 0] Model forward completed.
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 20] Time: 0.04s | Loss: 0.0003
[Rank 4] Model forward completed.
[Rank 0] Model forward completed.
[Rank 2] Model forward completed.[Rank 1] Model forward completed.

[Rank 3] Model forward completed.
[Epoch 21] Time: 0.04s | Loss: 0.0003
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 4] Model forward completed.
[Rank 3] Model forward completed.[Rank 0] Model forward completed.

[Epoch 22] Time: 0.04s | Loss: 0.0003
[Rank 3] Model forward completed.
[Rank 1] Model forward completed.
[Rank 2] Model forward completed.
[Rank 4] Model forward completed.
[Rank 0] Model forward completed.
[Epoch 23] Time: 0.04s | Loss: 0.0003
[Rank 1] Model forward completed.
[Rank 2] Model forward completed.
[Rank 4] Model forward completed.
[Rank 0] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 24] Time: 0.04s | Loss: 0.0003
[Rank 1] Model forward completed.
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.[Rank 0] Model forward completed.

[Rank 3] Model forward completed.
[Epoch 25] Time: 0.04s | Loss: 0.0003
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.
[Rank 0] Model forward completed.
[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 26] Time: 0.04s | Loss: 0.0003
[Rank 2] Model forward completed.
[Rank 4] Model forward completed.
[Rank 0] Model forward completed.
[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 27] Time: 0.04s | Loss: 0.0003
[Rank 2] Model forward completed.
[Rank 4] Model forward completed.
[Rank 1] Model forward completed.
[Rank 0] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 28] Time: 0.04s | Loss: 0.0003
[Rank 0] Model forward completed.
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 29] Time: 0.04s | Loss: 0.0003
Epoch 30, batch size: 1000
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.
[Rank 0] Model forward completed.
[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 30] Time: 0.04s | Loss: 0.0003
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Rank 0] Model forward completed.
[Epoch 31] Time: 0.04s | Loss: 0.0003
[Rank 1] Model forward completed.
[Rank 0] Model forward completed.[Rank 4] Model forward completed.

[Rank 3] Model forward completed.[Rank 2] Model forward completed.

[Epoch 32] Time: 0.04s | Loss: 0.0003
[Rank 0] Model forward completed.
[Rank 3] Model forward completed.
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 4] Model forward completed.
[Epoch 33] Time: 0.04s | Loss: 0.0003
[Rank 2] Model forward completed.
[Rank 3] Model forward completed.
[Rank 0] Model forward completed.
[Rank 1] Model forward completed.
[Rank 4] Model forward completed.
[Epoch 34] Time: 0.04s | Loss: 0.0003
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 0] Model forward completed.[Rank 3] Model forward completed.

[Epoch 35] Time: 0.04s | Loss: 0.0002
[Rank 0] Model forward completed.
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.
[Rank 3] Model forward completed.
[Rank 1] Model forward completed.
[Epoch 36] Time: 0.05s | Loss: 0.0002
[Rank 2] Model forward completed.
[Rank 4] Model forward completed.
[Rank 3] Model forward completed.
[Rank 0] Model forward completed.
[Rank 1] Model forward completed.
[Epoch 37] Time: 0.05s | Loss: 0.0002
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.
[Rank 3] Model forward completed.
[Rank 0] Model forward completed.
[Rank 1] Model forward completed.
[Epoch 38] Time: 0.04s | Loss: 0.0002
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.
[Rank 0] Model forward completed.
[Rank 3] Model forward completed.
[Rank 1] Model forward completed.
[Epoch 39] Time: 0.04s | Loss: 0.0002
Epoch 40, batch size: 1000
[Rank 4] Model forward completed.
[Rank 0] Model forward completed.
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 40] Time: 0.04s | Loss: 0.0002
[Rank 4] Model forward completed.
[Rank 0] Model forward completed.[Rank 3] Model forward completed.

[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Epoch 41] Time: 0.04s | Loss: 0.0002
[Rank 2] Model forward completed.
[Rank 4] Model forward completed.
[Rank 0] Model forward completed.[Rank 3] Model forward completed.

[Rank 1] Model forward completed.
[Epoch 42] Time: 0.04s | Loss: 0.0002
[Rank 2] Model forward completed.
[Rank 0] Model forward completed.
[Rank 3] Model forward completed.
[Rank 4] Model forward completed.
[Rank 1] Model forward completed.
[Epoch 43] Time: 0.04s | Loss: 0.0002
[Rank 4] Model forward completed.
[Rank 0] Model forward completed.
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Epoch 44] Time: 0.04s | Loss: 0.0002
[Rank 4] Model forward completed.
[Rank 0] Model forward completed.
[Rank 3] Model forward completed.
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Epoch 45] Time: 0.04s | Loss: 0.0002
[Rank 4] Model forward completed.
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 3] Model forward completed.
[Rank 0] Model forward completed.
[Epoch 46] Time: 0.05s | Loss: 0.0002
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 4] Model forward completed.
[Rank 3] Model forward completed.
[Rank 0] Model forward completed.
[Epoch 47] Time: 0.04s | Loss: 0.0002
[Rank 2] Model forward completed.
[Rank 1] Model forward completed.
[Rank 4] Model forward completed.
[Rank 3] Model forward completed.
[Rank 0] Model forward completed.
[Epoch 48] Time: 0.06s | Loss: 0.0002
[Rank 1] Model forward completed.
[Rank 2] Model forward completed.
[Rank 3] Model forward completed.
[Rank 4] Model forward completed.
[Rank 0] Model forward completed.
[Epoch 49] Time: 0.05s | Loss: 0.0002
[Rank 2] Model forward completed.
[Rank 3] Model forward completed.
[Rank 1] Model forward completed.
[Rank 4] Model forward completed.
Epoch 50, batch size: 1000
[Rank 0] Model forward completed.
[Epoch 50] Time: 0.05s | Loss: 0.0002
Traceback (most recent call last):
  File "/home/sc.uni-leipzig.de/ci72buri/projects/vae/Leipzig_big_data_praktikum_gene_expression_autoencoder/train_vae.py", line 180, in <module>
    train()
  File "/home/sc.uni-leipzig.de/ci72buri/projects/vae/Leipzig_big_data_praktikum_gene_expression_autoencoder/train_vae.py", line 173, in train
    evaluate_and_print_reconstructions(model.module, dataloader, device)
  File "/home/sc.uni-leipzig.de/ci72buri/projects/vae/Leipzig_big_data_praktikum_gene_expression_autoencoder/train_vae.py", line 52, in evaluate_and_print_reconstructions
    (recon, _), mu, logvar = model(data, tau0)
ValueError: too many values to unpack (expected 2)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/sc.uni-leipzig.de/ci72buri/projects/vae/Leipzig_big_data_praktikum_gene_expression_autoencoder/train_vae.py", line 180, in <module>
[rank0]:     train()
[rank0]:   File "/home/sc.uni-leipzig.de/ci72buri/projects/vae/Leipzig_big_data_praktikum_gene_expression_autoencoder/train_vae.py", line 173, in train
[rank0]:     evaluate_and_print_reconstructions(model.module, dataloader, device)
[rank0]:   File "/home/sc.uni-leipzig.de/ci72buri/projects/vae/Leipzig_big_data_praktikum_gene_expression_autoencoder/train_vae.py", line 52, in evaluate_and_print_reconstructions
[rank0]:     (recon, _), mu, logvar = model(data, tau0)
[rank0]: ValueError: too many values to unpack (expected 2)
paula11:21278:21850 [4] NCCL INFO comm 0x55ccf1cbb560 rank 4 nranks 5 cudaDev 4 busId c1000 - Destroy COMPLETE
paula11:21277:21849 [3] NCCL INFO comm 0x56160d12eb10 rank 3 nranks 5 cudaDev 3 busId a1000 - Destroy COMPLETE
paula11:21275:21848 [1] NCCL INFO comm 0x55e5a2f127b0 rank 1 nranks 5 cudaDev 1 busId 22000 - Destroy COMPLETE
paula11:21276:21851 [2] NCCL INFO comm 0x562e02b74b40 rank 2 nranks 5 cudaDev 2 busId 61000 - Destroy COMPLETE
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mstellar-eon-192[0m at: [34mhttps://wandb.ai/coyfelix7-universit-t-leipzig/big_data_vae/runs/ex8byn5q[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250714_235511-ex8byn5q/logs[0m
E0714 23:57:26.628000 21246 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 21274) of binary: /home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/bin/python3.10
Traceback (most recent call last):
  File "/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
  File "/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/sc.uni-leipzig.de/ci72buri/.conda/envs/cell-vae/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_vae.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-14_23:57:26
  host      : paula11.sc.uni-leipzig.de
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 21274)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
